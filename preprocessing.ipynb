{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c23205-c345-41d0-a75b-cd5367d8431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import cml.data_v1 as cmldata\n",
    "\n",
    "\n",
    "db_name= os.environ[\"DBNAME\"]\n",
    "tbl_name = []\n",
    "\n",
    "#db_path =   \"s3a://vr-uat1/warehouse/tablespace/external/hive\"\n",
    "#path_prefix = \"s3a://vr-uat1/data/bank/\"\n",
    "\n",
    "db_path = os.environ[\"DBPATH\"]\n",
    "path_prefix = os.environ[\"PATHPREFIX\"]\n",
    "CONNECTION_NAME = os.environ[\"CONNECTION_NAME\"]\n",
    "tbl_name = []\n",
    "\n",
    "#db_name= \"vr_\" + \"testdb\"\n",
    "#db_path =   \"s3a://vr-uat1/warehouse/tablespace/external/hive\"\n",
    "\n",
    "# spark = (SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"bank-demo\")\n",
    "#     .config(\"spark.sql.warehouse.dir\", db_path)\n",
    "#     .config(\"spark.hadoop.fs.s2a.s3guard.ddb.region\", \"us-east-1\")\n",
    "#     .config('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "#     .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://vr-uat1/\")\n",
    "#     .master(\"local[5]\") # should be possible to change this to SPARK on Yarn or SPARK on Kubernetes\n",
    "#     .getOrCreate())\n",
    "\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470fdebb-f7d0-44fd-8db6-b27ac929d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='airlines', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/airlines.db'),\n",
       " Database(name='airlines_csv', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/airlines_csv.db'),\n",
       " Database(name='dbt_hive_demo', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo.db'),\n",
       " Database(name='dbt_hive_demo1', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo1.db'),\n",
       " Database(name='dbt_hive_demo1_mart_covid', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo1_mart_covid.db'),\n",
       " Database(name='dbt_hive_demo1_reference', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo1_reference.db'),\n",
       " Database(name='dbt_hive_demo1_staging_covid', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo1_staging_covid.db'),\n",
       " Database(name='dbt_hive_demo_mart_covid', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo_mart_covid.db'),\n",
       " Database(name='dbt_hive_demo_reference', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo_reference.db'),\n",
       " Database(name='dbt_hive_demo_staging_covid', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dbt_hive_demo_staging_covid.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive'),\n",
       " Database(name='dtag_demo', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/dtag_demo.db'),\n",
       " Database(name='information_schema', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/information_schema.db'),\n",
       " Database(name='sys', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/sys.db'),\n",
       " Database(name='vr_testdb', description='', locationUri='s3a://goes-se-sandbox01/warehouse/tablespace/external/hive/vr_testdb.db')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8d6556-4150-4635-9f61-88c9ccdd8cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|vr_testdb|            trans|      false|\n",
      "|vr_testdb|             loan|      false|\n",
      "|vr_testdb|          account|      false|\n",
      "|vr_testdb|             card|      false|\n",
      "|vr_testdb|           client|      false|\n",
      "|vr_testdb|           orders|      false|\n",
      "|vr_testdb|      disposition|      false|\n",
      "|vr_testdb|         district|      false|\n",
      "|vr_testdb|    trans_cleaned|      false|\n",
      "|vr_testdb|loan_full_cleaned|      false|\n",
      "|vr_testdb|    order_cleaned|      false|\n",
      "|vr_testdb|   client_cleaned|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES FROM {db_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf6c1a2-b40e-48a4-917f-eceeefab5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1056320|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) from {db_name}.trans\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76eff3-f0b6-417c-8cce-303c2a82de16",
   "metadata": {},
   "source": [
    "## Objective : We want to segment the customer base to identify the High value customers\n",
    "\n",
    "** References - [\"Research Paper\"](https://www.researchgate.net/profile/Herna-Viktor/publication/228949135_Mining_relational_databases_with_multi-view_learning/links/0fcfd506d9766253e1000000/Mining-relational-databases-with-multi-view-learning.pdf?origin=publication_detail )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06395c6-eae2-4433-b7fc-9e8fc47129b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|status|count(1)|\n",
      "+------+--------+\n",
      "|     B|      31|\n",
      "|     D|      45|\n",
      "|     C|     403|\n",
      "|     A|     203|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select status, count(*) from {db_name}.loan group by status\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de16b1a-13de-4333-afaf-4bcbf32721a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data model \n",
    "![](./images/DataModel.png?raw=true \"Data  Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47be5c-4d60-4cde-ad09-7b6c842fba17",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "**Entity-Relationship Description **\n",
    "- Each account has both static characteristics (e.g. date of creation, address of the branch) given in relation \"account\" and dynamic characteristics (e.g. payments debited or credited, balances) given in relations \"permanent order\" and \"transaction\".\n",
    "- Relation \"client\" describes characteristics of persons who can manipulate with the accounts.\n",
    "- One client can have more accounts, more clients can manipulate with single account; clients and accounts are related together in relation \"disposition\".\n",
    "- Relations \"loan\" and \"credit card\" describe some services which the bank offers to its clients;\n",
    "- More than one credit card can be issued to an account,\n",
    "- At most one loan can be granted for an account.\n",
    "- Relation \"demographic data\" gives some publicly available information about the districts (e.g. the unemployment rate); additional information about the clients can be deduced from this\n",
    "\n",
    "[credits: webpages.charlotte.edu](https://webpages.charlotte.edu/mirsad/itcs6265/group1/domain.html)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843db5-74f7-4d7f-94a4-370c20ce4d82",
   "metadata": {},
   "source": [
    "## ( Machine ) Learning Problem Description: \n",
    "### Classification of the loans \n",
    "Total : 682 Records, with status reflecting status of the loans\n",
    "- A: Finished but good\n",
    "- B: Finished but Bad\n",
    "- C: good but not finished- \n",
    "- D: Bad and not finished\n",
    "\n",
    "\n",
    "\n",
    "Background of every loan can be identified using Account, Client, Order,Transaction, Card, Disposition and District. \n",
    "\n",
    "Learning Problems we will work on here : \n",
    "1. If a loan is good or bad from 234 Finished laons\n",
    "2. Use this to classify risk on the other existing Loans the loan is good or bad from 682 instances \n",
    "\n",
    "\n",
    "### Predict likelihood of accepting Credit Cards ( e.g. if we run a marketing campaign how likely is the user likely to accept credit cards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d697a7-204a-499b-af42-b1a35a94577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+---------+\n",
      "|disp_id|client_id|account_id|     type|\n",
      "+-------+---------+----------+---------+\n",
      "|      1|        1|         1|    OWNER|\n",
      "|      2|        2|         2|    OWNER|\n",
      "|      3|        3|         2|DISPONENT|\n",
      "|      4|        4|         3|    OWNER|\n",
      "|      5|        5|         3|DISPONENT|\n",
      "|      6|        6|         4|    OWNER|\n",
      "|      7|        7|         5|    OWNER|\n",
      "|      8|        8|         6|    OWNER|\n",
      "|      9|        9|         7|    OWNER|\n",
      "|     10|       10|         8|    OWNER|\n",
      "|     11|       11|         8|DISPONENT|\n",
      "|     12|       12|         9|    OWNER|\n",
      "|     13|       13|        10|    OWNER|\n",
      "|     14|       14|        11|    OWNER|\n",
      "|     15|       15|        12|    OWNER|\n",
      "|     16|       16|        12|DISPONENT|\n",
      "|     17|       17|        13|    OWNER|\n",
      "|     18|       18|        13|DISPONENT|\n",
      "|     19|       19|        14|    OWNER|\n",
      "|     20|       20|        15|    OWNER|\n",
      "+-------+---------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {db_name}.disposition\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c681125-cc72-42c5-9b7f-322fb01e7d21",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Quality Checks.\n",
    "\n",
    "** Business Rules usually set up by a Business Analyst/SME \n",
    "- Ensure Every Account has an Owner  ( i.e. a record in Account table also has one in Owner)\n",
    "- Ensure Order / Loan exists in Transactions ( i.e. Records found in Order and Loan tables are also existing Transactions table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98d3931-80c2-4b02-a1dc-5c3dc0fe3fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+----+----------+\n",
      "|account_id|district_id|frequency|date|account_id|\n",
      "+----------+-----------+---------+----+----------+\n",
      "+----------+-----------+---------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write simple SQL query to check that \n",
    "\n",
    "# Checking that there are no orphaned account IDs (without Owners)\n",
    "sql_string = f\"\"\"\n",
    "    SELECT A.*, B.account_id  FROM {db_name}.account A\n",
    "        LEFT OUTER JOIN {db_name}.disposition B ON A.account_id = B.account_id WHERE B.account_id IS NULL \"\"\"\n",
    "\n",
    "spark.sql(sql_string).show()\n",
    "\n",
    "## You can remove IS NULL above to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c538b603-8c12-4393-8e2e-d987e4aa87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------+--------+--------+------+----------+-----+\n",
      "|loan_id|account_id|date|amount|duration|payments|status|account_id|total|\n",
      "+-------+----------+----+------+--------+--------+------+----------+-----+\n",
      "+-------+----------+----+------+--------+--------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that Every record in Order and Loan table also exists as a transaction\n",
    "spark.sql(f\"\"\"\n",
    "    \n",
    "           SELECT loan_ds.loan_id, loan_ds.account_id, loan_ds.date, loan_ds.amount, loan_ds.duration, loan_ds.payments, loan_ds.status, trans_ds.account_id,ROUND(trans_ds.total,0 ) as total FROM {db_name}.loan AS loan_ds\n",
    "            LEFT OUTER JOIN \n",
    "          ( SELECT  DISTINCT account_id, sum(amount) as total  FROM  {db_name}.trans WHERE K_symbol = 'UVER'  GROUP BY account_Id , balance) AS  trans_ds\n",
    "         ON loan_ds.account_id = trans_ds.account_id\n",
    "         WHERE trans_ds.account_id IS NULL     \n",
    "          \"\"\").show()\n",
    "\n",
    "# Uncomment the line WHERE trans_ds.account_id ISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8abd9005-68d7-4a44-a2d3-9f332eab5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|Tablename|account_Id|Total|\n",
      "+---------+----------+-----+\n",
      "|    trans|account_Id|    0|\n",
      "|    trans|      date|    0|\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do some basic null checks on key tables\n",
    "df_trans_acct = spark.sql(\n",
    "    f\"\"\" \n",
    "        SELECT \"trans\" as Tablename , \"account_Id\", count(account_id) AS Total FROM {db_name}.trans WHERE account_id IS NULL\n",
    "    \"\"\")\n",
    "df_trans_date = spark.sql(\n",
    "    f\"\"\" \n",
    "        SELECT \"trans\" as Tablename , \"date\", count(date) AS Total FROM {db_name}.trans WHERE date IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "df_trans_acct.union(df_trans_date).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131a7125-8975-4f69-998d-fd73c0f15ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------------+\n",
      "|card_id|disp_id|   type|         issued|\n",
      "+-------+-------+-------+---------------+\n",
      "|   1005|   9285|classic|931107 00:00:00|\n",
      "|    104|    588|classic|940119 00:00:00|\n",
      "|    747|   4915|classic|940205 00:00:00|\n",
      "|     70|    439|classic|940208 00:00:00|\n",
      "|    577|   3687|classic|940215 00:00:00|\n",
      "|    377|   2429|classic|940303 00:00:00|\n",
      "|    721|   4680| junior|940405 00:00:00|\n",
      "|    437|   2762|classic|940601 00:00:00|\n",
      "|    188|   1146|classic|940619 00:00:00|\n",
      "|     13|     87|classic|940629 00:00:00|\n",
      "|    732|   4763|classic|940721 00:00:00|\n",
      "|    181|   1066|classic|940819 00:00:00|\n",
      "|    384|   2475| junior|940915 00:00:00|\n",
      "|    309|   1946|classic|940919 00:00:00|\n",
      "|    478|   3084|classic|941009 00:00:00|\n",
      "|    563|   3589|classic|941021 00:00:00|\n",
      "|    369|   2363|classic|941108 00:00:00|\n",
      "|    376|   2428|classic|941110 00:00:00|\n",
      "|    483|   3115|classic|941124 00:00:00|\n",
      "|    174|   1039| junior|941204 00:00:00|\n",
      "+-------+-------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {db_name}.card\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1065c1-74e4-4aa6-a0af-9deacbf4a1f3",
   "metadata": {},
   "source": [
    "### Preprocessing Step 1 : \n",
    "Examples of pre-processing issues that we now have to handle on the data\n",
    "- Client Information : Extract age, gender \n",
    "- Transaction Table : Convert Transaction Information (type, operation , ksymbol)\n",
    "- Convert Date Types\n",
    "\n",
    "### Preprocessing Step 2: \n",
    "We will create 2 Data sets ( Loan and Credit card ) to then do some data analysis on these 2 types of businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b4c131-3258-494c-ad05-482a12f0bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+\n",
      "|client_id|gender|birth_number|district_id|\n",
      "+---------+------+------------+-----------+\n",
      "|        1|Female|      706213|         18|\n",
      "|        2|  Male|      450204|          1|\n",
      "|        3|Female|      406009|          1|\n",
      "|        4|  Male|      561201|          5|\n",
      "|        5|  Male|      605703|          5|\n",
      "|        6|  Male|      190922|         12|\n",
      "|        7|Female|      290125|         15|\n",
      "|        8|Female|      385221|         51|\n",
      "|        9|Female|      351016|         60|\n",
      "|       10|  Male|      430501|         57|\n",
      "|       11|Female|      505822|         57|\n",
      "|       12|  Male|      810220|         40|\n",
      "|       13|  Male|      745529|         54|\n",
      "|       14|Female|      425622|         76|\n",
      "|       15|Female|      185828|         21|\n",
      "|       16|  Male|      190225|         21|\n",
      "|       17|  Male|      341013|         76|\n",
      "|       18|  Male|      315405|         76|\n",
      "|       19|  Male|      421228|         47|\n",
      "|       20|Female|      790104|         46|\n",
      "+---------+------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Step 1: \n",
    "client_df = spark.sql(f\"SELECT * from {db_name}.client\")\n",
    "client_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89add22a-66bd-4c60-8d9b-aa8d2fdd91b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+----------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|\n",
      "+---------+------+------------+-----------+----------+\n",
      "|        1|Female|      706213|         18|1970-12-13|\n",
      "|        2|  Male|      450204|          1|1945-02-04|\n",
      "|        3|Female|      406009|          1|1940-10-09|\n",
      "|        4|  Male|      561201|          5|1956-12-01|\n",
      "|        5|  Male|      605703|          5|1960-07-03|\n",
      "|        6|  Male|      190922|         12|1919-09-22|\n",
      "|        7|Female|      290125|         15|1929-01-25|\n",
      "|        8|Female|      385221|         51|1938-02-21|\n",
      "|        9|Female|      351016|         60|1935-10-16|\n",
      "|       10|  Male|      430501|         57|1943-05-01|\n",
      "|       11|Female|      505822|         57|1950-08-22|\n",
      "|       12|  Male|      810220|         40|1981-02-20|\n",
      "|       13|  Male|      745529|         54|1974-05-29|\n",
      "|       14|Female|      425622|         76|1942-06-22|\n",
      "|       15|Female|      185828|         21|1918-08-28|\n",
      "|       16|  Male|      190225|         21|1919-02-25|\n",
      "|       17|  Male|      341013|         76|1934-10-13|\n",
      "|       18|  Male|      315405|         76|1931-04-05|\n",
      "|       19|  Male|      421228|         47|1942-12-28|\n",
      "|       20|Female|      790104|         46|1979-01-04|\n",
      "+---------+------+------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Birth number is an encoded variable containing Birthday and Sex: \n",
    "# Value : YYMMDD ( for Men)\n",
    "# Value YYMM+50DD ( for Women)\n",
    "\n",
    "# let us apply Spark Transform function here \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, BooleanType, DateType, IntegerType\n",
    "\n",
    "\n",
    "# client_df.withColumn(\"newb\", when(col(\"gender\") == \"Female\" & col(\"birth_number\").substr(3,2) > 50 , col(\"birth_number\") - 5000).otherwise(col(\"birth_number\"))).show()\n",
    "client_df = client_df.withColumn(\"birth_numberU\", \n",
    "#          when((col(\"gender\") == \"Female\") & (col(\"birth_number\").substr(3,2) > 50), col(\"birth_number\") - 5000)\n",
    "          when((col(\"birth_number\").substr(3,2) > 50), col(\"birth_number\") - 5000)\n",
    "         .otherwise( col(\"birth_number\")).cast(IntegerType()))\n",
    "\n",
    "client_df = client_df.withColumn(\"birth_date\", to_date(concat(lit(\"19\"), col(\"birth_numberU\").cast(StringType())), \"yyyyMMdd\"))\n",
    "client_df = client_df.drop(col(\"birth_numberU\"))\n",
    "client_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abbbe31b-663c-4715-83ee-d4e4a68ec4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+----------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|\n",
      "+---------+------+------------+-----------+----------+\n",
      "|        0|     0|           0|          0|         0|\n",
      "+---------+------+------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We see that we have some issues with data\n",
    "df2 = client_df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull(), c \\\n",
    "                           )).alias(c)\n",
    "                    for c in client_df.columns])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "064da192-f55d-461f-aebf-8385cde363c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+---------+\n",
      "|disp_id|client_id|account_id|     type|\n",
      "+-------+---------+----------+---------+\n",
      "|      1|        1|         1|    OWNER|\n",
      "|      2|        2|         2|    OWNER|\n",
      "|      3|        3|         2|DISPONENT|\n",
      "|      4|        4|         3|    OWNER|\n",
      "|      5|        5|         3|DISPONENT|\n",
      "|      6|        6|         4|    OWNER|\n",
      "|      7|        7|         5|    OWNER|\n",
      "|      8|        8|         6|    OWNER|\n",
      "|      9|        9|         7|    OWNER|\n",
      "|     10|       10|         8|    OWNER|\n",
      "+-------+---------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+----------+------+------+--------+--------+------+\n",
      "|loan_id|account_id|  date|amount|duration|payments|status|\n",
      "+-------+----------+------+------+--------+--------+------+\n",
      "|   5314|      1787|930705| 96396|      12| 8033.00|     B|\n",
      "|   5316|      1801|930711|165960|      36| 4610.00|     A|\n",
      "|   6863|      9188|930728|127080|      60| 2118.00|     A|\n",
      "|   5325|      1843|930803|105804|      36| 2939.00|     A|\n",
      "|   7240|     11013|930906|274740|      60| 4579.00|     A|\n",
      "|   6687|      8261|930913| 87840|      24| 3660.00|     A|\n",
      "|   7284|     11265|930915| 52788|      12| 4399.00|     A|\n",
      "|   6111|      5428|930924|174744|      24| 7281.00|     B|\n",
      "|   7235|     10973|931013|154416|      48| 3217.00|     A|\n",
      "|   5997|      4894|931104|117024|      24| 4876.00|     A|\n",
      "+-------+----------+------+------+--------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|disp_id| type|loan_id|account_id|  date|amount|duration|payments|status|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|     4959|  Male|      790425|         77|1979-04-25|   4959|OWNER|   4959|         2|940105| 80952|      24| 3373.00|     A|\n",
      "|     4961|  Male|      820731|         54|1982-07-31|   4961|OWNER|   4961|        19|960429| 30276|      12| 2523.00|     B|\n",
      "|     4968|  Male|      380526|         50|1938-05-26|   4968|OWNER|   4968|        38|980419|110736|      48| 2307.00|     C|\n",
      "|     4989|  Male|      350904|         28|1935-09-04|   4989|OWNER|   4989|       105|981205|352704|      48| 7348.00|     C|\n",
      "|     5002|Female|      775717|         65|1977-07-17|   5002|OWNER|   5002|       173|940531|104808|      12| 8734.00|     A|\n",
      "|     5046|Female|      370822|          1|1937-08-22|   5046|OWNER|   5046|       349|980909| 42816|      12| 3568.00|     C|\n",
      "|     5072|  Male|      815324|          1|1981-03-24|   5072|OWNER|   5072|       472|970428|196800|      24| 8200.00|     D|\n",
      "|     5117|  Male|      750321|         52|1975-03-21|   5117|OWNER|   5117|       718|970820| 76944|      12| 6412.00|     A|\n",
      "|     5126|  Male|      650403|         43|1965-04-03|   5126|OWNER|   5126|       790|940724|208128|      48| 4336.00|     B|\n",
      "|     5128|  Male|      631224|         70|1963-12-24|   5128|OWNER|   5128|       808|960411|215616|      48| 4492.00|     D|\n",
      "|     5132|Female|      671021|         51|1967-10-21|   5132|OWNER|   5132|       817|950217|538500|      60| 8975.00|     C|\n",
      "|     5134|Female|      675719|         58|1967-07-19|   5134|OWNER|   5134|       825|970202| 53076|      12| 4423.00|     A|\n",
      "|     5138|Female|      745805|         45|1974-08-05|   5138|OWNER|   5138|       854|950530| 87216|      48| 1817.00|     C|\n",
      "|     5145|Female|      506031|         26|1950-10-31|   5145|OWNER|   5145|       915|950916|309552|      48| 6449.00|     C|\n",
      "|     5189|Female|      615110|         74|1961-01-10|   5189|OWNER|   5189|      1166|940207|149040|      48| 3105.00|     A|\n",
      "|     5212|Female|      466012|         72|1946-10-12|   5212|OWNER|   5212|      1256|950529| 71064|      36| 1974.00|     A|\n",
      "|     5214|Female|      760912|         47|1976-09-12|   5214|OWNER|   5214|      1270|970701| 74772|      36| 2077.00|     C|\n",
      "|     5236|Female|      780603|         25|1978-06-03|   5236|OWNER|   5236|      1374|970915|389136|      48| 8107.00|     C|\n",
      "|     5237|  Male|      770803|          1|1977-08-03|   5237|OWNER|   5237|      1375|950829| 45336|      24| 1889.00|     A|\n",
      "|     5241|Female|      780626|         63|1978-06-26|   5241|OWNER|   5241|      1389|980902|125472|      24| 5228.00|     C|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let us now merge the client, disp and loans data\n",
    "disp_df = spark.sql(f\"SELECT * from {db_name}.disposition\")\n",
    "loan_df = spark.sql(f\"SELECT * FROM {db_name}.loan\")\n",
    "\n",
    "disp_df.show(10)\n",
    "loan_df.show(10)\n",
    "\n",
    "loan_full_df = (client_df.join(disp_df, client_df.client_id == disp_df.client_id , \"inner\")\n",
    "                         .join(loan_df, disp_df.disp_id == loan_df.loan_id, \"inner\")).drop(disp_df.client_id).drop(disp_df.account_id)\n",
    "loan_full_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85dabf03-380a-4dcf-9b32-33c2fc94391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "|trans_id|account_id|  date|  type|    operation| amount|balance|k_symbol|bank| account|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "|  695247|      2378|930101|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  171812|       576|930101|PRIJEM|        VKLAD| 900.00| 900.00|    null|null|    null|\n",
      "|  207264|       704|930101|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|\n",
      "| 1117247|      3818|930101|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|\n",
      "|  579373|      1972|930102|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|\n",
      "|  771035|      2632|930102|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|\n",
      "|  452728|      1539|930103|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|\n",
      "|  725751|      2484|930103|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|\n",
      "|  497211|      1695|930103|PRIJEM|        VKLAD| 200.00| 200.00|    null|null|    null|\n",
      "|  232960|       793|930103|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  505240|      1726|930103|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|\n",
      "|  144541|       485|930104|PRIJEM|        VKLAD| 300.00| 300.00|    null|null|    null|\n",
      "|  637741|      2177|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  689827|      2357|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  846006|      2881|930104|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  637742|      2177|930105|PRIJEM|PREVOD Z UCTU|5123.00|5923.00|  DUCHOD|  YZ|62457513|\n",
      "| 2908688|      9635|930105|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|\n",
      "|  232961|       793|930105|PRIJEM|PREVOD Z UCTU|3401.00|4201.00|    null|  IJ| 6149286|\n",
      "|  192096|       652|930105|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  542215|      1844|930106|PRIJEM|        VKLAD| 500.00| 500.00|    null|null|    null|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(f\"SELECT * FROM {db_name}.trans WHERE operation IS NOT NULL\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419645fe-4b81-414d-837e-02d17ffe0d66",
   "metadata": {},
   "source": [
    "### Transformation : We now need to transform the Type type, Operation, K_symbol fields to readable formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b91dfa-030a-457b-9084-0be04c004415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "@udf(returnType=StringType())    \n",
    "def convert_trans_type_to_eng(x):\n",
    "    if x == 'PRIJEM':\n",
    "        return 'Credit'\n",
    "    elif x == 'VYDAJ':\n",
    "        return 'Withdrawal'\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "@udf(returnType=StringType())    \n",
    "def convert_trans_op_to_eng(x):\n",
    "    if x == 'VYBER KARTOU':\n",
    "        return 'Credit card withdrawal'\n",
    "    elif x == 'VKLAD':\n",
    "        return 'Credit in cash'\n",
    "    elif x == 'PREVOD Z UCTU':\n",
    "        return 'Collection from another bank'\n",
    "    elif x == 'VYBER':\n",
    "        return 'Withdrawal in Cash'\n",
    "    elif x == 'PREVOD NA UCET':\n",
    "        return 'Remittance to another bank'    \n",
    "    else:\n",
    "        return np.NaN\n",
    "@udf(returnType=StringType())        \n",
    "def convert_trans_k_symbol_to_eng(x):\n",
    "    if x == 'POJISTNE':\n",
    "        return 'Insurance payment'\n",
    "    elif x == 'SLUZBY':\n",
    "        return 'Payment for statement'\n",
    "    elif x == 'UROK':\n",
    "        return 'Interest credited'\n",
    "    elif x == 'SANKC. UROK':\n",
    "        return 'Sanction interest if negative balance'\n",
    "    elif x == 'SIPO':\n",
    "        return 'Household'\n",
    "    elif x == 'DUCHOD':\n",
    "        return 'Old-age pension'  \n",
    "    elif x == 'UVER':\n",
    "        return 'Loan payment'      \n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e18482-cec6-4d77-a6c4-e7133e60eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "|trans_id|account_id|  date|  type|    operation| amount|balance|k_symbol|bank| account|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "|  695247|      2378|930101|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  171812|       576|930101|PRIJEM|        VKLAD| 900.00| 900.00|    null|null|    null|\n",
      "|  207264|       704|930101|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|\n",
      "| 1117247|      3818|930101|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|\n",
      "|  579373|      1972|930102|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|\n",
      "|  771035|      2632|930102|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|\n",
      "|  452728|      1539|930103|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|\n",
      "|  725751|      2484|930103|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|\n",
      "|  497211|      1695|930103|PRIJEM|        VKLAD| 200.00| 200.00|    null|null|    null|\n",
      "|  232960|       793|930103|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  505240|      1726|930103|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|\n",
      "|  144541|       485|930104|PRIJEM|        VKLAD| 300.00| 300.00|    null|null|    null|\n",
      "|  637741|      2177|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  689827|      2357|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|\n",
      "|  846006|      2881|930104|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  637742|      2177|930105|PRIJEM|PREVOD Z UCTU|5123.00|5923.00|  DUCHOD|  YZ|62457513|\n",
      "| 2908688|      9635|930105|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|\n",
      "|  232961|       793|930105|PRIJEM|PREVOD Z UCTU|3401.00|4201.00|    null|  IJ| 6149286|\n",
      "|  192096|       652|930105|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|\n",
      "|  542215|      1844|930106|PRIJEM|        VKLAD| 500.00| 500.00|    null|null|    null|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+----------+--------+--------------------+---------------+\n",
      "|trans_id|account_id|  date|  type|    operation| amount|balance|k_symbol|bank| account|trans_date|type_eng|       operation_eng|   k_symbol_eng|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+----------+--------+--------------------+---------------+\n",
      "|  695247|      2378|930101|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|1993-01-01|  Credit|      Credit in cash|            NaN|\n",
      "|  171812|       576|930101|PRIJEM|        VKLAD| 900.00| 900.00|    null|null|    null|1993-01-01|  Credit|      Credit in cash|            NaN|\n",
      "|  207264|       704|930101|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|1993-01-01|  Credit|      Credit in cash|            NaN|\n",
      "| 1117247|      3818|930101|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|1993-01-01|  Credit|      Credit in cash|            NaN|\n",
      "|  579373|      1972|930102|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|1993-01-02|  Credit|      Credit in cash|            NaN|\n",
      "|  771035|      2632|930102|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|1993-01-02|  Credit|      Credit in cash|            NaN|\n",
      "|  452728|      1539|930103|PRIJEM|        VKLAD| 600.00| 600.00|    null|null|    null|1993-01-03|  Credit|      Credit in cash|            NaN|\n",
      "|  725751|      2484|930103|PRIJEM|        VKLAD|1100.00|1100.00|    null|null|    null|1993-01-03|  Credit|      Credit in cash|            NaN|\n",
      "|  497211|      1695|930103|PRIJEM|        VKLAD| 200.00| 200.00|    null|null|    null|1993-01-03|  Credit|      Credit in cash|            NaN|\n",
      "|  232960|       793|930103|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|1993-01-03|  Credit|      Credit in cash|            NaN|\n",
      "|  505240|      1726|930103|PRIJEM|        VKLAD|1000.00|1000.00|    null|null|    null|1993-01-03|  Credit|      Credit in cash|            NaN|\n",
      "|  144541|       485|930104|PRIJEM|        VKLAD| 300.00| 300.00|    null|null|    null|1993-01-04|  Credit|      Credit in cash|            NaN|\n",
      "|  637741|      2177|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|1993-01-04|  Credit|      Credit in cash|            NaN|\n",
      "|  689827|      2357|930104|PRIJEM|        VKLAD| 800.00| 800.00|    null|null|    null|1993-01-04|  Credit|      Credit in cash|            NaN|\n",
      "|  846006|      2881|930104|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|1993-01-04|  Credit|      Credit in cash|            NaN|\n",
      "|  637742|      2177|930105|PRIJEM|PREVOD Z UCTU|5123.00|5923.00|  DUCHOD|  YZ|62457513|1993-01-05|  Credit|Collection from a...|Old-age pension|\n",
      "| 2908688|      9635|930105|PRIJEM|        VKLAD| 400.00| 400.00|    null|null|    null|1993-01-05|  Credit|      Credit in cash|            NaN|\n",
      "|  232961|       793|930105|PRIJEM|PREVOD Z UCTU|3401.00|4201.00|    null|  IJ| 6149286|1993-01-05|  Credit|Collection from a...|            NaN|\n",
      "|  192096|       652|930105|PRIJEM|        VKLAD| 700.00| 700.00|    null|null|    null|1993-01-05|  Credit|      Credit in cash|            NaN|\n",
      "|  542215|      1844|930106|PRIJEM|        VKLAD| 500.00| 500.00|    null|null|    null|1993-01-06|  Credit|      Credit in cash|            NaN|\n",
      "+--------+----------+------+------+-------------+-------+-------+--------+----+--------+----------+--------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "trans_df = spark.sql(f\"SELECT * FROM {db_name}.trans WHERE operation IS NOT NULL\")\n",
    "\n",
    "trans_df.show()\n",
    "\n",
    "trans_df = trans_df.withColumn(\"trans_date\",  to_date(concat(lit(\"19\"), col(\"date\").cast(StringType())), \"yyyyMMdd\")) \\\n",
    "        .select(\"trans_id\", \"account_id\", \"date\", \"type\", \"operation\", \"amount\", \"balance\", \"k_symbol\", \"bank\", \"account\", \"trans_date\", \\\n",
    "                        convert_trans_type_to_eng(col(\"type\")).alias(\"type_eng\" ), \\\n",
    "                        convert_trans_op_to_eng(col(\"operation\")).alias(\"operation_eng\"), \\\n",
    "                        convert_trans_k_symbol_to_eng(col(\"k_symbol\")).alias(\"k_symbol_eng\"))\n",
    "trans_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98f67244-2086-4af3-80a8-b4095fc74daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to Clean Order table as well \n",
    "order_df = spark.sql(f\"Select * from {db_name}.orders\")\n",
    "@udf(returnType=StringType())\n",
    "def convert_order_k_symbol_to_eng(x):\n",
    "    if x == 'POJISTNE':\n",
    "        return 'Insurance payment'\n",
    "    elif x == 'SIPO':\n",
    "        return 'Household'\n",
    "    elif x == 'LEASING':\n",
    "        return 'Leasing'\n",
    "    elif x == 'UVER':\n",
    "        return 'Loan payment'\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "order_df = order_df.withColumn(\"k_symbol_eng\",  convert_order_k_symbol_to_eng(col(\"k_symbol\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8413090-abdc-448e-b78a-e96f7d9deb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+\n",
      "|client_id|gender|birth_number|district_id|\n",
      "+---------+------+------------+-----------+\n",
      "|        1|Female|      706213|         18|\n",
      "|        2|  Male|      450204|          1|\n",
      "|        3|Female|      406009|          1|\n",
      "|        4|  Male|      561201|          5|\n",
      "|        5|  Male|      605703|          5|\n",
      "|        6|  Male|      190922|         12|\n",
      "|        7|Female|      290125|         15|\n",
      "|        8|Female|      385221|         51|\n",
      "|        9|Female|      351016|         60|\n",
      "|       10|  Male|      430501|         57|\n",
      "|       11|Female|      505822|         57|\n",
      "|       12|  Male|      810220|         40|\n",
      "|       13|  Male|      745529|         54|\n",
      "|       14|Female|      425622|         76|\n",
      "|       15|Female|      185828|         21|\n",
      "|       16|  Male|      190225|         21|\n",
      "|       17|  Male|      341013|         76|\n",
      "|       18|  Male|      315405|         76|\n",
      "|       19|  Male|      421228|         47|\n",
      "|       20|Female|      790104|         46|\n",
      "+---------+------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"REFRESH TABLE vr_testdb.client\").show()\n",
    "spark.sql(f\"SELECT * from {db_name}.client\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a485b1b6-ebc0-46cc-b5b3-7c0c5fa0c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+----------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|\n",
      "+---------+------+------------+-----------+----------+\n",
      "|        1|Female|      706213|         18|1970-12-13|\n",
      "|        2|  Male|      450204|          1|1945-02-04|\n",
      "|        3|Female|      406009|          1|1940-10-09|\n",
      "+---------+------+------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+----------+--------+--------------+------------+\n",
      "|trans_id|account_id|  date|  type|operation| amount|balance|k_symbol|bank|account|trans_date|type_eng| operation_eng|k_symbol_eng|\n",
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+----------+--------+--------------+------------+\n",
      "|  695247|      2378|930101|PRIJEM|    VKLAD| 700.00| 700.00|    null|null|   null|1993-01-01|  Credit|Credit in cash|         NaN|\n",
      "|  171812|       576|930101|PRIJEM|    VKLAD| 900.00| 900.00|    null|null|   null|1993-01-01|  Credit|Credit in cash|         NaN|\n",
      "|  207264|       704|930101|PRIJEM|    VKLAD|1000.00|1000.00|    null|null|   null|1993-01-01|  Credit|Credit in cash|         NaN|\n",
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+----------+--------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|disp_id| type|loan_id|account_id|  date|amount|duration|payments|status|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|     4959|  Male|      790425|         77|1979-04-25|   4959|OWNER|   4959|         2|940105| 80952|      24| 3373.00|     A|\n",
      "|     4961|  Male|      820731|         54|1982-07-31|   4961|OWNER|   4961|        19|960429| 30276|      12| 2523.00|     B|\n",
      "|     4968|  Male|      380526|         50|1938-05-26|   4968|OWNER|   4968|        38|980419|110736|      48| 2307.00|     C|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+----------+-------+--------+------------+\n",
      "|order_id|account_id|bank_to|account_to| amount|k_symbol|k_symbol_eng|\n",
      "+--------+----------+-------+----------+-------+--------+------------+\n",
      "|   29401|         1|     YZ|  87144583|2452.00|    SIPO|   Household|\n",
      "|   29402|         2|     ST|  89597016|3372.70|    UVER|Loan payment|\n",
      "|   29403|         2|     QR|  13943797|7266.00|    SIPO|   Household|\n",
      "+--------+----------+-------+----------+-------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finally We need to write all this data as a cleaned dataset\n",
    "(client_df.write \n",
    "          .mode(\"overwrite\")\n",
    "#          .option(\"path\", db_path )\n",
    "          .saveAsTable(db_name + \".\" + \"client_cleaned\", format=\"parquet\"))\n",
    "#spark.sql(\"REFRESH TABLE \" + db_name + \".\" + \"client_cleaned\")\n",
    "spark.sql(\"SELECT * from \" + db_name + \".\" + \"client_cleaned\").show(3)\n",
    "\n",
    "(trans_df.write \n",
    "           .mode(\"overwrite\")\n",
    "#           .option(\"path\", db_path )\n",
    "           .saveAsTable(db_name + \".\" + \"trans_cleaned\", format=\"parquet\"))\n",
    "# spark.sql(\"REFRESH TABLE \" + db_name + \".\" + \"trans_cleaned\")\n",
    "spark.sql(\"SELECT * from \" + db_name + \".\" + \"trans_cleaned\").show(3)\n",
    "\n",
    "(loan_full_df.write \n",
    "           .mode(\"overwrite\")\n",
    "#   .option(\"path\", db_path )\n",
    "           .saveAsTable(db_name + \".\" + \"loan_full_cleaned\", format=\"parquet\"))\n",
    "# #    spark.sql(\"REFRESH TABLE \" + db_name + \".\" + table_name).show()\n",
    "spark.sql(\"SELECT * from \" + db_name + \".\" + \"loan_full_cleaned\").show(3)\n",
    "\n",
    "(order_df.write \n",
    "           .mode(\"overwrite\")\n",
    "#   .option(\"path\", db_path )\n",
    "           .saveAsTable(db_name + \".\" + \"order_cleaned\", format=\"parquet\"))\n",
    "# #    spark.sql(\"REFRESH TABLE \" + db_name + \".\" + table_name).show()\n",
    "spark.sql(\"SELECT * from \" + db_name + \".\" + \"order_cleaned\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3279125a-e00b-4a3d-b961-da3204040d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e526e66-2508-49d3-bcf1-326dc847173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 8281c004-7d58-4e30-94fd-f65e0434401c\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|client_id|gender|birth_number|district_id|birth_date|disp_id| type|loan_id|account_id|  date|amount|duration|payments|status|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "|     4959|  Male|      790425|         77|1979-04-25|   4959|OWNER|   4959|         2|940105| 80952|      24| 3373.00|     A|\n",
      "|     4961|  Male|      820731|         54|1982-07-31|   4961|OWNER|   4961|        19|960429| 30276|      12| 2523.00|     B|\n",
      "|     4968|  Male|      380526|         50|1938-05-26|   4968|OWNER|   4968|        38|980419|110736|      48| 2307.00|     C|\n",
      "|     4989|  Male|      350904|         28|1935-09-04|   4989|OWNER|   4989|       105|981205|352704|      48| 7348.00|     C|\n",
      "|     5002|Female|      775717|         65|1977-07-17|   5002|OWNER|   5002|       173|940531|104808|      12| 8734.00|     A|\n",
      "|     5046|Female|      370822|          1|1937-08-22|   5046|OWNER|   5046|       349|980909| 42816|      12| 3568.00|     C|\n",
      "|     5072|  Male|      815324|          1|1981-03-24|   5072|OWNER|   5072|       472|970428|196800|      24| 8200.00|     D|\n",
      "|     5117|  Male|      750321|         52|1975-03-21|   5117|OWNER|   5117|       718|970820| 76944|      12| 6412.00|     A|\n",
      "|     5126|  Male|      650403|         43|1965-04-03|   5126|OWNER|   5126|       790|940724|208128|      48| 4336.00|     B|\n",
      "|     5128|  Male|      631224|         70|1963-12-24|   5128|OWNER|   5128|       808|960411|215616|      48| 4492.00|     D|\n",
      "|     5132|Female|      671021|         51|1967-10-21|   5132|OWNER|   5132|       817|950217|538500|      60| 8975.00|     C|\n",
      "|     5134|Female|      675719|         58|1967-07-19|   5134|OWNER|   5134|       825|970202| 53076|      12| 4423.00|     A|\n",
      "|     5138|Female|      745805|         45|1974-08-05|   5138|OWNER|   5138|       854|950530| 87216|      48| 1817.00|     C|\n",
      "|     5145|Female|      506031|         26|1950-10-31|   5145|OWNER|   5145|       915|950916|309552|      48| 6449.00|     C|\n",
      "|     5189|Female|      615110|         74|1961-01-10|   5189|OWNER|   5189|      1166|940207|149040|      48| 3105.00|     A|\n",
      "|     5212|Female|      466012|         72|1946-10-12|   5212|OWNER|   5212|      1256|950529| 71064|      36| 1974.00|     A|\n",
      "|     5214|Female|      760912|         47|1976-09-12|   5214|OWNER|   5214|      1270|970701| 74772|      36| 2077.00|     C|\n",
      "|     5236|Female|      780603|         25|1978-06-03|   5236|OWNER|   5236|      1374|970915|389136|      48| 8107.00|     C|\n",
      "|     5237|  Male|      770803|          1|1977-08-03|   5237|OWNER|   5237|      1375|950829| 45336|      24| 1889.00|     A|\n",
      "|     5241|Female|      780626|         63|1978-06-26|   5241|OWNER|   5241|      1389|980902|125472|      24| 5228.00|     C|\n",
      "+---------+------+------------+-----------+----------+-------+-----+-------+----------+------+------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#check to see if the spark tables are persisted in the datalake. \n",
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = os.environ[\"CONNECTION_NAME\"]\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "spark.sql(f\"select * from {db_name}.loan_full_cleaned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5955408e-fabf-4c9f-b0f2-c2496909e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c4105-5ae9-491c-8e2d-5241b3529c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
